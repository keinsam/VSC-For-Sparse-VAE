[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Report - Variational Sparse Coding",
    "section": "",
    "text": "0.1 Datasets",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#datasets",
    "href": "index.html#datasets",
    "title": "Report - Variational Sparse Coding",
    "section": "",
    "text": "MNIST : TODO…\nFashion-MNIST: A collection of 28x28 grayscale images of clothing items (e.g., T-shirts, trousers), comprising 60,000 training and 10,000 test samples. It serves as a drop-in replacement for MNIST, offering richer variability for testing generative models.\nSmiley : TODO…\nUCI HAR: A dataset of accelerometer and gyroscope signals from smartphones, capturing six human activities (e.g., walking, sitting) across 30 subjects, with preprocessed segments of 128 time steps.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#report-structure",
    "href": "index.html#report-structure",
    "title": "Report - Variational Sparse Coding",
    "section": "0.2 Report Structure",
    "text": "0.2 Report Structure\n\nAutoencoders: Introduces the basics of autoencoders and their role in high-dimensional data analysis.\nVariational Autoencoders: Explains VAEs, focusing on the Evidence Lower Bound (ELBO) and latent space regularization.\nVariational Sparse Coding: Details the VSC model, including the Spike and Slab prior and warm-up training strategy.\nExperiments: Summarizes empirical results, comparing VSC to β-VAEs across multiple tasks.\nConclusion: Highlights VSC’s contributions and future directions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "autoencoder.html",
    "href": "autoencoder.html",
    "title": "2  Auto Encoder",
    "section": "",
    "text": "2.1 Architecture\n\\[\n\\mathcal{L}_{\\text{AE}} = \\text{Reconstruction\\_term} = \\|x - \\text{Decoder}(\\text{Encoder}(x))\\|^2\n\\]\nPytorch implementation: logic/model/autoencoder.py",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Auto Encoder</span>"
    ]
  },
  {
    "objectID": "autoencoder.html#architecture",
    "href": "autoencoder.html#architecture",
    "title": "2  Auto Encoder",
    "section": "",
    "text": "Encoder: Maps input \\(x \\in \\mathbb{R}^D\\) to latent code \\(z \\in \\mathbb{R}^d\\) (compression).\nDecoder: Reconstructs \\(\\hat{x}\\) from \\(z\\) (reconstruction).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Auto Encoder</span>"
    ]
  },
  {
    "objectID": "autoencoder.html#applications",
    "href": "autoencoder.html#applications",
    "title": "2  Auto Encoder",
    "section": "2.2 Applications",
    "text": "2.2 Applications\n\nFeature extraction for clustering/visualization.\nDenoising (e.g., noisy Fashion-MNIST reconstruction).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Auto Encoder</span>"
    ]
  },
  {
    "objectID": "autoencoder.html#limitations",
    "href": "autoencoder.html#limitations",
    "title": "2  Auto Encoder",
    "section": "2.3 Limitations",
    "text": "2.3 Limitations\n\nDeterministic: No probabilistic latent space.\nNo control over latent structure (e.g., disentanglement).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Auto Encoder</span>"
    ]
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "3  Variational Auto Encoder",
    "section": "",
    "text": "Variational Autoencoders (VAEs) extend autoencoders into a probabilistic framework, enabling both representation learning and generative modeling.\nUnlike traditional autoencoders, VAEs model the latent variables \\(z\\) as drawn from a distribution, typically a Gaussian, parameterized by the encoder.\n\n3.0.1 VAE’s latent space\n\nThe encoder outputs parameters \\(\\mu\\) and \\(\\sigma^2\\)\n\n\\(q_\\phi(z|x) = \\mathcal{N}(z; \\mu, \\sigma^2)\\).\n\nThe decoder generates \\(p_\\theta(x|z)\\).\nThe latent space prior is typically a standard Gaussian,\n\n\\(p(z) = \\mathcal{N}(0, I)\\), which constrains the latent space to remain centered around the origin, promoting meaningful structure for visualization and generative tasks.\n\nThe goal is to maximize the marginal likelihood \\(p(x)\\), but this is intractable due to the integral:\n\n\\[ p(x) = \\int p_\\theta(x|z) p(z) dz \\]\n\n\n3.0.2 Evidence Lower Bound (ELBO)\nVAEs approximate the marginal likelihood \\(p(x)\\) using the Evidence Lower Bound (ELBO):\n\\[ \\mathcal{L}_{\\text{ELBO}} = \\mathbb{E}_{q_\\phi(z|x)} [\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x) \\| p(z)) \\]\n\nReconstruction Term: \\(\\mathbb{E}_{q_\\phi(z|x)} [\\log p_\\theta(x|z)]\\)\n\nencourages accurate data reconstruction.\n\nKL Divergence Term: \\(D_{\\text{KL}}(q_\\phi(z|x) \\| p(z))\\)\n\nregularizes the latent distribution to match the prior.\n\n\nThe ELBO is optimized with respect to encoder parameters \\(\\phi\\) and decoder parameters \\(\\theta\\) using the reparameterization trick for gradient computation.\ndef reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n\n\n3.0.3 \\(\\beta\\)-VAE\nThe β-VAE introduces a hyperparameter \\(\\beta\\) to control the weight of the KL divergence term in the ELBO:\n\\[ \\mathcal{L}_{\\beta\\text{-VAE}} = \\mathbb{E}_{q_\\phi(z|x)} [\\log p_\\theta(x|z)] - \\beta D_{\\text{KL}}(q_\\phi(z|x) \\| p(z)) \\]\n\nWhen \\(\\beta = 0\\), the \\(\\beta\\)-VAE reduces to a simple autoencoder model.\nWhen \\(\\beta = 1\\), the \\(\\beta\\)-VAE reduces to the standard VAE.\nIncreasing \\(\\beta\\) encourages disentanglement in the latent space, making it more interpretable but potentially at the cost of reconstruction quality.\n\nThe \\(\\beta\\)-VAE is particularly useful in scenarios where interpretability of the latent space is critical, such as in disentangled representation learning. However, careful tuning of \\(\\beta\\) is required to balance reconstruction and disentanglement.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variational Auto Encoder</span>"
    ]
  },
  {
    "objectID": "vsc.html",
    "href": "vsc.html",
    "title": "4  Variational Sparse Coding",
    "section": "",
    "text": "4.1 Motivation\nWhat is Posterior Collapse?\nHow VSC Fixes It:\nResult:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational Sparse Coding</span>"
    ]
  },
  {
    "objectID": "vsc.html#motivation",
    "href": "vsc.html#motivation",
    "title": "4  Variational Sparse Coding",
    "section": "",
    "text": "Problem: In VAEs, some latent dimensions become “useless” – they encode no meaningful information.\n\nWhy it happens:\n\nThe KL divergence term in ELBO forces latent variables to match the prior.\nIf the decoder is too powerful, it ignores latent variables, leading to dimensions being permanently inactive.\n\nResult: Model uses only a few dimensions, losing sparsity and disentanglement.\n\n\n\n\nSpike-and-Slab Warm-Up\n\nPhase 1 (\\(\\lambda=0\\)):\n\nForces latent variables to behave like binary codes (spike dominates).\n\nModel must “choose” which dimensions to activate (no continuous refinement).\n\n\nPhase 2 (\\(\\lambda \\rightarrow 1\\)):\n\nGradually introduces continuous slab parameters (\\(\\mu_{i,j}, \\sigma_{i,j}\\)).\n\nPrevents early over-reliance on a few dimensions.\n\n\nSparsity Enforcement\n\nKL Sparsity Term: Penalizes average activation rate \\(\\bar{\\gamma}_u\\) if it deviates from \\(\\alpha\\) (e.g., \\(\\alpha=0.01\\)).\n\nForces the model to use only essential dimensions, avoiding redundancy.\n\nDynamic Prior\n\nPrior \\(p_s(z)\\) adapts via pseudo-inputs \\(x_u\\) and classifier \\(C_\\omega(x_i)\\).\n\nPrevents trivial alignment with a fixed prior (e.g., \\(\\mathcal{N}(0,1)\\)).\n\n\n\n\nLatent dimensions stay sparse and interpretable.\n\nNo single dimension dominates; features are distributed across active variables. Variational Sparse Coding (VSC) extends VAEs by inducing sparsity in the latent space using a Spike-and-Slab prior, enabling feature disentanglement and controlled generation when the number of latent factors is unknown.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational Sparse Coding</span>"
    ]
  },
  {
    "objectID": "vsc.html#recognition-model",
    "href": "vsc.html#recognition-model",
    "title": "4  Variational Sparse Coding",
    "section": "4.2 Recognition Model",
    "text": "4.2 Recognition Model\nSpike-and-Slab Encoder Distribution\n\\[ q_\\phi(z|x_i) = \\prod_{j=1}^J \\left[ \\gamma_{i,j} \\mathcal{N}(z_{i,j}; \\mu_{i,j}, \\sigma_{i,j}^2) + (1 - \\gamma_{i,j}) \\delta(z_{i,j}) \\right] \\]\nParameters: All outputs of a neural network (encoder).\n\n\\(\\gamma_{i,j}\\): Probability that latent dimension \\(j\\) is active for input \\(x_i\\).\n\n\\(\\mu_{i,j}, \\sigma_{i,j}\\): Mean and variance of the Gaussian (slab) for active dimensions.\n\n\\(\\delta(z_{i,j})\\): Dirac delta function (spike) forces inactive dimensions to exactly 0.\n\nPytorch implementation of the reparameterization logic/model/vsc.py:\ndef reparameterize(self, \n    mu: torch.Tensor, \n    logvar: torch.Tensor, \n    gamma: torch.Tensor\n    ) -&gt; torch.Tensor:\n    \n    lamb = self.lambda_val  # warm-up factor\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n\n    # Interpolate between a fixed (zero-mean, unit variance) slab \n    # and the learned slab.\n    slab = lam * mu + eps * (lam * std + (1 - lam))\n    \n    # Sample binary spike; note: torch.bernoulli is not differentiable.\n    spike = torch.bernoulli(gamma)\n    \n    return spike * slab",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational Sparse Coding</span>"
    ]
  },
  {
    "objectID": "vsc.html#training-procedure",
    "href": "vsc.html#training-procedure",
    "title": "4  Variational Sparse Coding",
    "section": "4.3 Training Procedure",
    "text": "4.3 Training Procedure\n\n4.3.1 Prior Distribution & Objective\nPrior\n\\[ p_s(z) = q_\\phi(z|x_{u^*}), \\quad u^* = C_\\omega(x_i) \\]\n\nPseudo-inputs: Learnable templates \\(\\{x_u\\}\\) represent common feature combinations.\n\nClassifier: \\(C_\\omega(x_i)\\) selects the best-matching template \\(x_{u^*}\\) for input \\(x_i\\).\n\nObjective (ELBO with Sparsity)\n\\[ \\mathcal{L} = \\sum_i \\left[ -\\text{KL}(q_\\phi \\| p_s) + \\mathbb{E}_{q_\\phi}[\\log p_\\theta(x_i|z)] \\right] - J \\cdot \\text{KL}(\\bar{\\gamma}_u \\| \\alpha) \\]\n\nKL Term:\n\nAligns encoder (\\(\\mu_{i,j}, \\sigma_{i,j}, \\gamma_{i,j}\\)) with prior (\\(\\mu_{u^*,j}, \\sigma_{u^*,j}, \\gamma_{u^*,j}\\)).\n\nClosed-form formula ensures fast computation.\n\n\nSparsity Term:\n\nPenalizes deviation from target sparsity \\(\\alpha\\) (e.g., 90% dimensions inactive).\n\n\nPytorch implementation in logic/model/vsc.py:\ndef compute_sparsity_loss(self, gamma: torch.Tensor) -&gt; torch.Tensor:\n    target = torch.full_like(gamma, self.prior_sparsity)\n    return nn.functional.binary_cross_entropy(gamma, target)\n\n\n4.3.2 Warm-Up Strategy\n\\[ q_{\\phi,\\lambda}(z|x_i) = \\prod_{j=1}^J \\left[ \\gamma_{i,j} \\mathcal{N}(z_{i,j}; \\lambda \\mu_{i,j}, \\lambda \\sigma_{i,j}^2 + (1-\\lambda)) + (1 - \\gamma_{i,j}) \\delta(z_{i,j}) \\right] \\]\n\nPhase 1 (\\(\\lambda=0\\)):\n\nSlab fixed to \\(\\mathcal{N}(0,1)\\) (binary-like latent codes).\n\nFocus: Which features to activate.\n\n\nPhase 2 (\\(\\lambda \\rightarrow 1\\)):\n\nGradually learn \\(\\mu_{i,j}, \\sigma_{i,j}\\) (refine how to represent features).\n\n\nAvoids collapse: Prevents premature “freezing” of latent dimensions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational Sparse Coding</span>"
    ]
  }
]