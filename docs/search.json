[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Report - Variational Sparse Coding",
    "section": "",
    "text": "0.1 Datasets",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#datasets",
    "href": "index.html#datasets",
    "title": "Report - Variational Sparse Coding",
    "section": "",
    "text": "MNIST :\n\nA dataset of 28×28 grayscale images of handwritten digits (0–9), consisting of 60,000 training samples and 10,000 test samples. It is widely used as a benchmark for image classification and serves as a foundational dataset for testing generative models and neural network architectures.\n\nFashion-MNIST:\n\nA collection of 28x28 grayscale images of clothing items (e.g., T-shirts, trousers), comprising 60,000 training and 10,000 test samples. It serves as a drop-in replacement for MNIST, offering richer variability for testing generative models.\n\nSmiley (Used in the original paper) :\n\nA dataset of 28×28 grayscale images depicting simple smiley faces with varying expressions. Designed as a toy dataset, it is useful for evaluating generative and classification models on abstract, low-complexity visual data.\n\nUCI HAR (Used in the original paper):\n\nA dataset of accelerometer and gyroscope signals from smartphones, capturing six human activities (e.g., walking, sitting) across 30 subjects, with preprocessed segments of 128 time steps.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#report-structure",
    "href": "index.html#report-structure",
    "title": "Report - Variational Sparse Coding",
    "section": "0.2 Report Structure",
    "text": "0.2 Report Structure\n\nAutoencoders: Introduces the basics of autoencoders and their role in high-dimensional data analysis.\nVariational Autoencoders: Explains VAEs, focusing on the Evidence Lower Bound (ELBO) and latent space regularization.\nVariational Sparse Coding: Details the VSC model, including the Spike and Slab prior and warm-up training strategy.\nExperiments: Summarizes empirical results, comparing VSC to β-VAEs across multiple tasks.\nConclusion: Highlights VSC’s contributions and future directions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "autoencoder.html",
    "href": "autoencoder.html",
    "title": "2  Auto Encoder",
    "section": "",
    "text": "2.1 Architecture\n\\[\n\\mathcal{L}_{\\text{AE}} = \\text{Reconstruction\\_term} = \\|x - \\text{Decoder}(\\text{Encoder}(x))\\|^2\n\\]\nPytorch implementation: logic/model/autoencoder.py",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Auto Encoder</span>"
    ]
  },
  {
    "objectID": "autoencoder.html#architecture",
    "href": "autoencoder.html#architecture",
    "title": "2  Auto Encoder",
    "section": "",
    "text": "Encoder: Maps input \\(x \\in \\mathbb{R}^D\\) to latent code \\(z \\in \\mathbb{R}^d\\) (compression).\nDecoder: Reconstructs \\(\\hat{x}\\) from \\(z\\) (reconstruction).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Auto Encoder</span>"
    ]
  },
  {
    "objectID": "autoencoder.html#applications",
    "href": "autoencoder.html#applications",
    "title": "2  Auto Encoder",
    "section": "2.2 Applications",
    "text": "2.2 Applications\n\nFeature extraction for clustering/visualization.\nDenoising (e.g., noisy Fashion-MNIST reconstruction).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Auto Encoder</span>"
    ]
  },
  {
    "objectID": "autoencoder.html#limitations",
    "href": "autoencoder.html#limitations",
    "title": "2  Auto Encoder",
    "section": "2.3 Limitations",
    "text": "2.3 Limitations\n\nDeterministic: No probabilistic latent space.\nNo control over latent structure (e.g., disentanglement).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Auto Encoder</span>"
    ]
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "3  Variational Auto Encoder",
    "section": "",
    "text": "Variational Autoencoders (VAEs) extend autoencoders into a probabilistic framework, enabling both representation learning and generative modeling.\nUnlike traditional autoencoders, VAEs model the latent variables \\(z\\) as drawn from a distribution, typically a Gaussian, parameterized by the encoder.\n\n3.0.1 VAE’s latent space\n\nThe encoder outputs parameters \\(\\mu\\) and \\(\\sigma^2\\)\n\n\\(q_\\phi(z|x) = \\mathcal{N}(z; \\mu, \\sigma^2)\\).\n\nThe decoder generates \\(p_\\theta(x|z)\\).\nThe latent space prior is typically a standard Gaussian,\n\n\\(p(z) = \\mathcal{N}(0, I)\\), which constrains the latent space to remain centered around the origin, promoting meaningful structure for visualization and generative tasks.\n\nThe goal is to maximize the marginal likelihood \\(p(x)\\), but this is intractable due to the integral:\n\n\\[ p(x) = \\int p_\\theta(x|z) p(z) dz \\]\n\n\n3.0.2 Evidence Lower Bound (ELBO)\nVAEs approximate the marginal likelihood \\(p(x)\\) using the Evidence Lower Bound (ELBO):\n\\[ \\mathcal{L}_{\\text{ELBO}} = \\mathbb{E}_{q_\\phi(z|x)} [\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x) \\| p(z)) \\]\n\nReconstruction Term: \\(\\mathbb{E}_{q_\\phi(z|x)} [\\log p_\\theta(x|z)]\\)\n\nencourages accurate data reconstruction.\n\nKL Divergence Term: \\(D_{\\text{KL}}(q_\\phi(z|x) \\| p(z))\\)\n\nregularizes the latent distribution to match the prior.\n\n\nThe ELBO is optimized with respect to encoder parameters \\(\\phi\\) and decoder parameters \\(\\theta\\) using the reparameterization trick for gradient computation.\ndef reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n\n\n3.0.3 \\(\\beta\\)-VAE\nThe β-VAE introduces a hyperparameter \\(\\beta\\) to control the weight of the KL divergence term in the ELBO:\n\\[ \\mathcal{L}_{\\beta\\text{-VAE}} = \\mathbb{E}_{q_\\phi(z|x)} [\\log p_\\theta(x|z)] - \\beta D_{\\text{KL}}(q_\\phi(z|x) \\| p(z)) \\]\n\nWhen \\(\\beta = 0\\), the \\(\\beta\\)-VAE reduces to a simple autoencoder model.\nWhen \\(\\beta = 1\\), the \\(\\beta\\)-VAE reduces to the standard VAE.\nIncreasing \\(\\beta\\) encourages disentanglement in the latent space, making it more interpretable but potentially at the cost of reconstruction quality.\n\nThe \\(\\beta\\)-VAE is particularly useful in scenarios where interpretability of the latent space is critical, such as in disentangled representation learning. However, careful tuning of \\(\\beta\\) is required to balance reconstruction and disentanglement. Also, \\(\\beta\\)-VAE often work under the asumption that all the target features are present in all the data points.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variational Auto Encoder</span>"
    ]
  },
  {
    "objectID": "vsc.html",
    "href": "vsc.html",
    "title": "4  Variational Sparse Coding",
    "section": "",
    "text": "4.1 Motivation\nWe would want a model that has an interpretable latent space (by introducing sparcity) with more general feature disentenglement that \\(\\beta\\)-VAE, meaning that different combinations of features can be present in different data points.\nWhat is Posterior Collapse?\nHow VSC Fixes It:\nResult:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational Sparse Coding</span>"
    ]
  },
  {
    "objectID": "vsc.html#motivation",
    "href": "vsc.html#motivation",
    "title": "4  Variational Sparse Coding",
    "section": "",
    "text": "Problem: In VAEs, some latent dimensions become “useless” – they encode no meaningful information.\n\nWhy it happens:\n\nThe KL divergence term in ELBO forces latent variables to match the prior.\nIf the decoder is too powerful, it ignores latent variables, leading to dimensions being permanently inactive.\n\nResult: Model uses only a few dimensions, losing sparsity and disentanglement.\n\n\n\n\nSpike-and-Slab Warm-Up\n\nPhase 1 (\\(\\lambda=0\\)):\n\nForces latent variables to behave like binary codes (spike dominates).\n\nModel must “choose” which dimensions to activate (no continuous refinement).\n\n\nPhase 2 (\\(\\lambda \\rightarrow 1\\)):\n\nGradually introduces continuous slab parameters (\\(\\mu_{i,j}, \\sigma_{i,j}\\)).\n\nPrevents early over-reliance on a few dimensions.\n\n\nSparsity Enforcement\n\nKL Sparsity Term: Penalizes average activation rate \\(\\bar{\\gamma}_u\\) if it deviates from \\(\\alpha\\) (e.g., \\(\\alpha=0.01\\)).\n\nForces the model to use only essential dimensions, avoiding redundancy.\n\nDynamic Prior\n\nPrior \\(p_s(z)\\) adapts via pseudo-inputs \\(x_u\\) and classifier \\(C_\\omega(x_i)\\).\n\nPrevents trivial alignment with a fixed prior (e.g., \\(\\mathcal{N}(0,1)\\)).\n\n\n\n\nLatent dimensions stay sparse and interpretable.\n\nNo single dimension dominates; features are distributed across active variables. Variational Sparse Coding (VSC) extends VAEs by inducing sparsity in the latent space using a Spike-and-Slab prior, enabling feature disentanglement and controlled generation when the number of latent factors is unknown.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational Sparse Coding</span>"
    ]
  },
  {
    "objectID": "vsc.html#recognition-model",
    "href": "vsc.html#recognition-model",
    "title": "4  Variational Sparse Coding",
    "section": "4.2 Recognition Model",
    "text": "4.2 Recognition Model\nSpike-and-Slab Encoder Distribution\n\\[ q_\\phi(z|x_i) = \\prod_{j=1}^J \\left[ \\gamma_{i,j} \\mathcal{N}(z_{i,j}; \\mu_{i,j}, \\sigma_{i,j}^2) + (1 - \\gamma_{i,j}) \\delta(z_{i,j}) \\right] \\]\nParameters: All outputs of a neural network (encoder).\n\n\\(\\gamma_{i,j}\\): Probability that latent dimension \\(j\\) is active for input \\(x_i\\).\n\n\\(\\mu_{i,j}, \\sigma_{i,j}\\): Mean and variance of the Gaussian (slab) for active dimensions.\n\n\\(\\delta(z_{i,j})\\): Dirac delta function (spike) forces inactive dimensions to exactly 0.\n\nPytorch implementation of the reparameterization logic/model/vsc.py:\ndef reparameterize(self, \n    mu: torch.Tensor, \n    logvar: torch.Tensor, \n    gamma: torch.Tensor\n    ) -&gt; torch.Tensor:\n    \n    lamb = self.lambda_val  # warm-up factor\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n\n    # Interpolate between a fixed (zero-mean, unit variance) slab \n    # and the learned slab.\n    slab = lam * mu + eps * (lam * std + (1 - lam))\n    \n    # Sample binary spike; note: torch.bernoulli is not differentiable.\n    spike = torch.bernoulli(gamma)\n    \n    return spike * slab",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational Sparse Coding</span>"
    ]
  },
  {
    "objectID": "vsc.html#training-procedure",
    "href": "vsc.html#training-procedure",
    "title": "4  Variational Sparse Coding",
    "section": "4.3 Training Procedure",
    "text": "4.3 Training Procedure\n\n4.3.1 Prior Distribution & Objective\nPrior\n\\[ p_s(z) = q_\\phi(z|x_{u^*}), \\quad u^* = C_\\omega(x_i) \\]\n\nPseudo-inputs: Learnable templates \\(\\{x_u\\}\\) represent common feature combinations.\n\nClassifier: \\(C_\\omega(x_i)\\) selects the best-matching template \\(x_{u^*}\\) for input \\(x_i\\).\n\nObjective (ELBO with Sparsity)\n\\[ \\mathcal{L} = \\sum_i \\left[ -\\text{KL}(q_\\phi \\| p_s) + \\mathbb{E}_{q_\\phi}[\\log p_\\theta(x_i|z)] \\right] - J \\cdot \\text{KL}(\\bar{\\gamma}_u \\| \\alpha) \\]\n\nKL Term:\n\nAligns encoder (\\(\\mu_{i,j}, \\sigma_{i,j}, \\gamma_{i,j}\\)) with prior (\\(\\mu_{u^*,j}, \\sigma_{u^*,j}, \\gamma_{u^*,j}\\)).\n\nClosed-form formula ensures fast computation.\n\n\nSparsity Term:\n\nPenalizes deviation from target sparsity \\(\\alpha\\) (e.g., 90% dimensions inactive).\n\n\nPytorch implementation in logic/model/vsc.py:\ndef compute_sparsity_loss(self, gamma: torch.Tensor) -&gt; torch.Tensor:\n    target = torch.full_like(gamma, self.prior_sparsity)\n    return nn.functional.binary_cross_entropy(gamma, target)\n\n\n4.3.2 Warm-Up Strategy\n\\[ q_{\\phi,\\lambda}(z|x_i) = \\prod_{j=1}^J \\left[ \\gamma_{i,j} \\mathcal{N}(z_{i,j}; \\lambda \\mu_{i,j}, \\lambda \\sigma_{i,j}^2 + (1-\\lambda)) + (1 - \\gamma_{i,j}) \\delta(z_{i,j}) \\right] \\]\n\nPhase 1 (\\(\\lambda=0\\)):\n\nSlab fixed to \\(\\mathcal{N}(0,1)\\) (binary-like latent codes).\n\nFocus: Which features to activate.\n\n\nPhase 2 (\\(\\lambda \\rightarrow 1\\)):\n\nGradually learn \\(\\mu_{i,j}, \\sigma_{i,j}\\) (refine how to represent features).\n\n\nAvoids collapse: Prevents premature “freezing” of latent dimensions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variational Sparse Coding</span>"
    ]
  },
  {
    "objectID": "experiments.html",
    "href": "experiments.html",
    "title": "5  Experiments",
    "section": "",
    "text": "5.1 Latent Space Visualization\nTo qualitatively evaluate the structure of the latent space, we sample from a 2D grid of latent vectors and each point is decoded to an image\nThis provides insight into how the model organizes information in the latent space.\nAE: Exhibits minimal structure, with random digit distribution and no discernible clustering.\nVAE: Better organized and gradual transitions between digits.\nVSC: Should reveal discrete clusters for each digit class but we don’t see distinct boundaries like expected with the sparse coding framework.\nVSC-WU: Should show improved transition smoothness between clusters.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "experiments.html#activated-dimensions",
    "href": "experiments.html#activated-dimensions",
    "title": "5  Experiments",
    "section": "5.2 Activated Dimensions",
    "text": "5.2 Activated Dimensions\nWe visualize how each latent dimension is activated across the dataset categories. For each class (0 to 9), we compute the mean latent vector and display it as a heatmap.\nThis helps us identify how interpretable and sparse the representations are.\n   \nInterpretation:\nAE: Shows dense activations across all dimensions, indicating non-selective feature encoding.\nVAE: More balanced but still overlapping activations. No clear sparsity pattern.\nVSC: Demonstrates clear sparsity. The two dimensions remain near zero, and not the same are selectively active depending on digits. This aligns with the spike-and-slab prior’s goal of feature disentanglement. More latent dimensions could help demonstrate posterior collapse.\nVSC-WU: Same as for VSC but with lower and more homogeonous values. More latent dimensions could help demonstrate that the warp-up strategy helps to avoid posterior collapse but it is not obvious here.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "experiments.html#reconstruction",
    "href": "experiments.html#reconstruction",
    "title": "5  Experiments",
    "section": "5.3 Reconstruction",
    "text": "5.3 Reconstruction\nFinally, we evaluate reconstruction quality by comparing recontructions of original and noisy images.\nThis assesses both reconstruction quality and robustness to perturbations.\n   \nInterpretation:\nAE: Generates sharp reconstructions but fails to denoise effectively.\nVAE: Produces more robust reconstructions with partial noise suppression.\nVSC: Low quality reconstructions.\nVSC-WU: Low quality reconstructions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "6  Conclusion",
    "section": "",
    "text": "7 Conclusion\nThis report explored Variational Sparse Coding (VSC), an extension of Variational Autoencoders that introduces sparsity in the latent space via a Spike-and-Slab prior. Through theoretical analysis and empirical validation on MNIST, we demonstrated how VSC addresses key limitations of traditional VAEs and β-VAEs, particularly in achieving interpretable and controllable latent representations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#key-contributions-of-vsc",
    "href": "conclusion.html#key-contributions-of-vsc",
    "title": "6  Conclusion",
    "section": "7.1 Key Contributions of VSC",
    "text": "7.1 Key Contributions of VSC\nSparse Latent Representations: The Spike-and-Slab prior enforces sparsity by allowing latent dimensions to be exactly zero with high probability, unlike Gaussian priors in standard VAEs. This leads to disentangled features.\nDynamic Prior Adaptation: The prior is learned via pseudo-inputs and a classifier, avoiding the rigid assumptions of a fixed Gaussian prior. This enables the model to adapt to varying feature combinations across data points, a limitation of β-VAEs.\nWarm-Up Strategy: The two-phase training (binary-like and continuous refinement) prevents posterior collapse, ensuring latent dimensions remain active and interpretable.\nSparsity Control:\nThe KL sparsity term explicitly penalizes deviations from a target sparsity level, promoting efficient use of latent capacity.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#limitations",
    "href": "conclusion.html#limitations",
    "title": "6  Conclusion",
    "section": "7.2 Limitations",
    "text": "7.2 Limitations\nDiscrete vs. Continuous Sparsity: The Spike-and-Slab prior assumes hard sparsity (exact zeros). For some tasks, soft sparsity (e.g., Laplace prior) may be more appropriate.\nTask-Specific Tuning: The warm-up schedule and sparsity target \\(\\alpha\\) require careful tuning.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  }
]